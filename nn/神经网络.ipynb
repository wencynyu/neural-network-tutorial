{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a6b184d-af11-42e9-8033-06165447f642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "操作系统及版本信息:Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.35\n",
      "系统位数:('64bit', 'ELF')\n",
      "pytorch版本:2.3.0\n",
      "cuda版本:12.1\n",
      "cudnn版本:8902\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "import torch\n",
    "\n",
    "def showinfo(tip, info):\n",
    "    print(\"{}:{}\".format(tip,info))\n",
    "\n",
    "showinfo(\"操作系统及版本信息\",platform.platform())\n",
    "showinfo('系统位数', platform.architecture())\n",
    "showinfo('pytorch版本', torch.__version__)\n",
    "showinfo('cuda版本', torch.version.cuda)\n",
    "showinfo('cudnn版本', torch.backends.cudnn.version())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5874ef01-982e-43e7-abb5-255861e67136",
   "metadata": {},
   "source": [
    "## 根据理论基础note中的导数简单实现一个神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70f71704-764a-4de6-b0d5-b1bba7fcfa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "\n",
    "# 激活函数及其导数\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, weight=None, bias=None, activate_func=sigmoid, activate_func_derivative=sigmoid_derivative):\n",
    "        self.weight = weight if weight is not None else random.uniform(-1, 1)\n",
    "        self.bias = bias if bias is not None else random.uniform(-1, 1)\n",
    "        self.activate_func = activate_func\n",
    "        self.activate_func_derivative = activate_func_derivative\n",
    "        self.output = None\n",
    "        self.inputs = None\n",
    "        self.delta = None\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = self.activate_func(sum(inputs[i] * self.weight[i] for i in range(len(inputs))) + self.bias)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, error, learning_rate):\n",
    "        self.delta = error * self.activate_func_derivative(self.output)\n",
    "        for i in range(len(self.weight)):\n",
    "            self.weight[i] += learning_rate * self.delta * self.inputs[i]\n",
    "        self.bias += learning_rate * self.delta\n",
    "        return self.delta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa7708da-c907-404f-b5d6-49fe38b59c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.hidden_layer = [Node(weight=[random.uniform(-1, 1) for _ in range(input_size)]) for _ in range(hidden_size)]\n",
    "        self.output_layer = [Node(weight=[random.uniform(-1, 1) for _ in range(hidden_size)], activate_func=lambda x: x,\n",
    "                                  activate_func_derivative=lambda x: 1) for _ in range(output_size)]\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden_outputs = [node.forward(x) for node in self.hidden_layer]\n",
    "        final_outputs = [node.forward(hidden_outputs) for node in self.output_layer]\n",
    "        return final_outputs\n",
    "\n",
    "    def backward(self, y, learning_rate):\n",
    "        output_errors = [yi - yi_pred.output for yi, yi_pred in zip(y, self.output_layer)]\n",
    "        hidden_errors = [node.backward(error, learning_rate) for node, error in zip(self.output_layer, output_errors)]\n",
    "        for node in self.hidden_layer:\n",
    "            node.backward(sum(hidden_errors), learning_rate)\n",
    "\n",
    "    def train(self, x, y, epochs, learning_rate):\n",
    "        for epoch in range(epochs):\n",
    "            for xi, yi in zip(x, y):\n",
    "                self.forward([xi])\n",
    "                self.backward([yi], learning_rate)\n",
    "            if epoch % 1000 == 0:\n",
    "                predictions = [self.forward([xi])[0] for xi in x]\n",
    "                loss = sum((yi - yi_pred) ** 2 for yi, yi_pred in zip(y, predictions)) / len(y)\n",
    "                print(f'Epoch {epoch}, Loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22b1424-62fc-42f9-bdbb-a5876dbe6c07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
