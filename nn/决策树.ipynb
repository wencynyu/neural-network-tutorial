{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d728d940-83b9-4d29-b701-7e26dfc86bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "操作系统及版本信息:Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.35\n",
      "系统位数:('64bit', 'ELF')\n",
      "pytorch版本:2.3.0\n",
      "cuda版本:12.1\n",
      "cudnn版本:8902\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "import torch\n",
    "\n",
    "def showinfo(tip, info):\n",
    "    print(\"{}:{}\".format(tip,info))\n",
    "\n",
    "showinfo(\"操作系统及版本信息\",platform.platform())\n",
    "showinfo('系统位数', platform.architecture())\n",
    "showinfo('pytorch版本', torch.__version__)\n",
    "showinfo('cuda版本', torch.version.cuda)\n",
    "showinfo('cudnn版本', torch.backends.cudnn.version())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4546618-2014-4df1-9e8b-623cee81cf19",
   "metadata": {},
   "source": [
    "## 信息论\n",
    "\n",
    "### 香农熵\n",
    "香农熵是信息论中的一个概念，用来衡量系统的混乱程度或者不确定性。在机器学习中，特别是决策树算法中，香农熵常被用来衡量数据集的纯度。纯度越高，数据集的香农熵越低。\n",
    "$$Entropy(X) = - \\sum_{i=1}^{n} p_i \\log_2(p_i)$$\n",
    "\n",
    "#### 示例\n",
    "假设一个数据集X，X包含三类样本，它们的分布如下：\n",
    "- 类别 A 占总数的 40%，\n",
    "- 类别 B 占总数的 30%，\n",
    "- 类别 C 占总数的 30%。\n",
    "\n",
    "则计算香农熵的过程如下：\n",
    "Entropy(X) = - (0.4*log2(0.4) + 0.3*log2(0.3) + 0.3*log2(0.3))\n",
    "\n",
    "### 基尼指数\n",
    "基尼指数是衡量一个数据集的不纯度的指标，它在决策树的构建过程中用于选择最优的特征和分裂点。基尼指数越小，数据集的纯度越高。\n",
    "$$\\text{Gini}(X) = 1 - \\sum_{i=1}^{n} (p_i)^2$$\n",
    "\n",
    "#### 示例\n",
    "假设一个数据集X，X包含两类样本，它们的分布如下：\n",
    "- 类别 A 占总数的 60%，\n",
    "- 类别 B 占总数的 40%。\n",
    "\n",
    "计算基尼指数的过程如下：\n",
    "Gini(X) = 1 - (0.6^2 + 0.4^2)\n",
    "\n",
    "## 决策树模型\n",
    "重复根据特征值，进行二分类，优化目标为基尼指数 or 香农熵最大化（信息混乱化）\n",
    "\n",
    "### 基于决策树的优化、迭代模型（略）\n",
    "XGBoost，CatBoost等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be66a649-1c9e-4e97-add6-92dbc9111a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def entropy(y):\n",
    "    \"\"\"计算熵\"\"\"\n",
    "    counts = Counter(y)\n",
    "    res = 0.0\n",
    "    for num in counts.values():\n",
    "        p = num / len(y)\n",
    "        res += -p * math.log2(p)  # 香农熵 E = - p * log(p)用来表示一个数据集的混乱度，在（0，1）内越小数据集越准确\n",
    "    return res\n",
    "\n",
    "\n",
    "def split_dataset(X, y, feature_index, threshold):\n",
    "    \"\"\"根据特征和阈值划分数据集\"\"\"\n",
    "    left_X, left_y = [], []\n",
    "    right_X, right_y = [], []\n",
    "\n",
    "    for i, sample in enumerate(X):\n",
    "        if sample[feature_index] <= threshold:\n",
    "            left_X.append(sample)\n",
    "            left_y.append(y[i])\n",
    "        else:\n",
    "            right_X.append(sample)\n",
    "            right_y.append(y[i])\n",
    "\n",
    "    return (tuple(left_X), tuple(left_y)), (tuple(right_X), tuple(right_y))\n",
    "\n",
    "class Node:\n",
    "\n",
    "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, value=None):\n",
    "        self.feature_index = feature_index  # 用于划分的特征索引\n",
    "        self.threshold = threshold  # 划分的阈值\n",
    "        self.left = left  # 左子树\n",
    "        self.right = right  # 右子树\n",
    "        self.value = value  # 叶节点的类别值\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "\n",
    "    def _best_split(self, X, y):\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "        best_score = 0  # 对熵是最大化\n",
    "\n",
    "        num_features = len(X[0])\n",
    "        # 遍历每个feature中的每一个value，按照这个阈值进行分类，算出香农熵，并且使其最大化\n",
    "        for feature_index in range(num_features):\n",
    "            thresholds = Counter([row[feature_index] for row in X]).keys()\n",
    "            for threshold in thresholds:\n",
    "                (X_left, y_left), (X_right, y_right) = split_dataset(X, y, feature_index, threshold)\n",
    "                if len(y_left) == 0 or len(y_right) == 0:\n",
    "                    continue\n",
    "\n",
    "                score = (len(y_left) / len(y)) * entropy(y_left) + (len(y_right) / len(y)) * entropy(y_right)\n",
    "\n",
    "                if score > best_score:\n",
    "                    best_feature = feature_index\n",
    "                    best_threshold = threshold\n",
    "                    best_score = score\n",
    "\n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        if len(Counter(y)) == 1:\n",
    "            return Node(value=y[0])\n",
    "\n",
    "        if self.max_depth is not None and depth >= self.max_depth:\n",
    "            most_common_label = Counter(y).most_common(1)[0][0]\n",
    "            return Node(value=most_common_label)\n",
    "\n",
    "        feature_index, threshold = self._best_split(X, y)\n",
    "        if feature_index is None:\n",
    "            most_common_label = Counter(y).most_common(1)[0][0]\n",
    "            return Node(value=most_common_label)\n",
    "\n",
    "        (X_left, y_left), (X_right, y_right) = split_dataset(X, y, feature_index, threshold)\n",
    "        left_node = self._build_tree(X_left, y_left, depth + 1)\n",
    "        right_node = self._build_tree(X_right, y_right, depth + 1)\n",
    "        return Node(feature_index=feature_index, threshold=threshold, left=left_node, right=right_node)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self._build_tree(X, y)\n",
    "\n",
    "    def _predict_one(self, node, x):\n",
    "        if node.value is not None:\n",
    "            return node.value\n",
    "\n",
    "        if x[node.feature_index] <= node.threshold:\n",
    "            return self._predict_one(node.left, x)\n",
    "        else:\n",
    "            return self._predict_one(node.right, x)\n",
    "\n",
    "    def predict(self, X):\n",
    "        res = []\n",
    "        for x in X:\n",
    "            tmp = self._predict_one(self.tree, x)\n",
    "            res.append(tmp)\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f88942-976c-4ce8-998b-036eacc5f26c",
   "metadata": {},
   "source": [
    "## 以鸢尾花分类为例调用训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6ab2c7c-e94c-4c44-ac1c-8ff275b9563e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My decision tree Accuracy: 0.7555555555555555\n",
      "SkLearn decision tree Accuracy: 0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# 加载数据集\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 使用自己的决策树构造\n",
    "clf = DecisionTree(max_depth=50)\n",
    "clf.fit(X_train, y_train)\n",
    "# 进行预测\n",
    "y_pred = clf.predict(X_test)\n",
    "# 评估模型\n",
    "print(\"My decision tree Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# 使用sklearn内置决策树算法\n",
    "clf = DecisionTreeClassifier(criterion='entropy', random_state=42)  # 也可以使用'gini'\n",
    "# 训练决策树\n",
    "clf.fit(X_train, y_train)\n",
    "# 进行预测\n",
    "y_pred = clf.predict(X_test)\n",
    "# 评估模型\n",
    "print(\"SkLearn decision tree Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1ec3da-bb66-426a-94cb-90eebf9e741c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
